{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b434ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07931222",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'last-fm'\n",
    "algorithm = 'als'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba94016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/johnhannebery/gitrepos/recsys/Data/last-fm\n",
      "Embed path: /Users/johnhannebery/gitrepos/recsys/embeds/last-fm/als\n",
      "Plots path: /Users/johnhannebery/gitrepos/recsys/plots/last-fm\n",
      "Score path: /Users/johnhannebery/gitrepos/recsys/scores/last-fm/als\n",
      "Results path: /Users/johnhannebery/gitrepos/recsys/results/last-fm/als\n"
     ]
    }
   ],
   "source": [
    "ds_path = os.path.join(os.getcwd(), f'Data/{dataset}')\n",
    "embed_path = os.path.join(os.getcwd(), f'embeds/{dataset}/{algorithm}')\n",
    "plot_path = os.path.join(os.getcwd(), f'plots/{dataset}')\n",
    "scores_path = os.path.join(os.getcwd(), f'scores/{dataset}/{algorithm}')\n",
    "results_path = os.path.join(os.getcwd(), f'results/{dataset}/{algorithm}')\n",
    "print(\"Dataset path:\", ds_path)\n",
    "print(\"Embed path:\", embed_path)\n",
    "print(\"Plots path:\", plot_path)\n",
    "print(\"Score path:\", scores_path)\n",
    "print(\"Results path:\", results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8e8f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_ratings(file_name):\n",
    "    user_dict = dict()\n",
    "    inter_mat = list()\n",
    "\n",
    "    lines = open(file_name, 'r').readlines()\n",
    "    for l in lines:\n",
    "        tmps = l.strip()\n",
    "        inters = [int(i) for i in tmps.split(' ')]\n",
    "\n",
    "        u_id, pos_ids = inters[0], inters[1:]\n",
    "        pos_ids = list(set(pos_ids))\n",
    "\n",
    "        for i_id in pos_ids:\n",
    "            inter_mat.append([u_id, i_id])\n",
    "\n",
    "        if len(pos_ids) > 0:\n",
    "            user_dict[u_id] = pos_ids\n",
    "    return np.array(inter_mat), user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71163e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_user_dict = _load_ratings(os.path.join(ds_path,'train.txt'))\n",
    "test, test_user_dict = _load_ratings(os.path.join(ds_path,'test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "536b9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'item_id']\n",
    "train = pd.DataFrame(train, columns = cols)\n",
    "test = pd.DataFrame(test, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67863fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['user_id'].isin(train['user_id'].tolist())]\n",
    "test = test[test['item_id'].isin(train['item_id'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99ac9ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/johnhannebery/gitrepos/recsys/embeds/last-fm/als/entity_embed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m user_embed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(embed_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_embed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m item_embed \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentity_embed.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_m1/lib/python3.8/site-packages/pandas/io/common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/johnhannebery/gitrepos/recsys/embeds/last-fm/als/entity_embed.csv'"
     ]
    }
   ],
   "source": [
    "user_embed = pd.read_csv(os.path.join(embed_path,'user_embed.csv'))\n",
    "item_embed = pd.read_csv(os.path.join(embed_path,'entity_embed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read scores\n",
    "scores_df = pd.read_csv(os.path.join(scores_path,'scores_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561598e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = train.groupby('item_id', as_index = False).agg({'user_id':'count'}).rename(columns={'user_id':'n_ratings'})\n",
    "item_counts['novelty'] = -np.log2(item_counts['n_ratings'] / train['user_id'].nunique())\n",
    "\n",
    "class metrics():\n",
    "  \n",
    "  @staticmethod\n",
    "  def dcg_at_k(r, k, method: int =1) -> float:\n",
    "    r_k = np.asfarray(r)[:k]\n",
    "    if r_k.size:\n",
    "        if method == 0:\n",
    "            return r_k[0] + np.sum(r_k[1:] / np.log2(np.arange(2, r_k.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r_k / np.log2(np.arange(2, r_k.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0\n",
    "\n",
    "class Metrics():\n",
    "  \n",
    "  def __init__(self, r: list, k: int, all_pos_num: int):\n",
    "    self.r = r\n",
    "    self.k = k\n",
    "    self.all_pos_num = all_pos_num\n",
    "  \n",
    "  def precision_at_k(self) -> float:\n",
    "      r_k = np.asarray(self.r)[:self.k]\n",
    "      return np.mean(r_k)\n",
    "\n",
    "\n",
    "  def dcg_at_k(self, reverse_r = False, method: int = 1) -> float:\n",
    "      if reverse_r == True:\n",
    "        r_k = np.asfarray(sorted(self.r, reverse=True))[:self.k]\n",
    "      else:\n",
    "        r_k = np.asfarray(self.r)[:self.k]\n",
    "      if r_k.size:\n",
    "          if method == 0:\n",
    "              return r_k[0] + np.sum(r_k[1:] / np.log2(np.arange(2, r_k.size + 1)))\n",
    "          elif method == 1:\n",
    "              return np.sum(r_k / np.log2(np.arange(2, r_k.size + 2)))\n",
    "          else:\n",
    "              raise ValueError('method must be 0 or 1.')\n",
    "      return 0\n",
    "\n",
    "\n",
    "#   def ndcg_at_k(self, method: int = 1, reverse_r = True) -> float:\n",
    "#       dcg_max = self.dcg_at_k(reverse_r = reverse_r, method = method)\n",
    "#       if not dcg_max:\n",
    "#           return 0.\n",
    "#       return self.dcg_at_k(self.r, self.k, method) / dcg_max\n",
    "    \n",
    "  def ndcg_at_k(self, method: int = 1) -> float:\n",
    "      dcg_max = metrics.dcg_at_k(sorted(self.r, reverse=True), self.k, method)\n",
    "      if not dcg_max:\n",
    "          return 0.\n",
    "      return metrics.dcg_at_k(self.r, self.k, method) / dcg_max\n",
    "\n",
    "\n",
    "  def recall_at_k(self) -> float:\n",
    "      r_k = np.asfarray(self.r)[:self.k]\n",
    "      return np.sum(r_k) / self.all_pos_num\n",
    "\n",
    "\n",
    "  def hit_at_k(self) -> float:\n",
    "      r_k = np.array(self.r)[:self.k]\n",
    "      if np.sum(r_k) > 0:\n",
    "          return 1.\n",
    "      else:\n",
    "          return 0.\n",
    "        \n",
    "  @staticmethod\n",
    "  def F1(pre: float, rec: float) -> float:\n",
    "      if pre + rec > 0:\n",
    "          return (2.0 * pre * rec) / (pre + rec)\n",
    "      else:\n",
    "          return 0.\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def calculate_diversity(item_ids, item_embed):\n",
    "  if len(item_ids) == 1:\n",
    "    return 1\n",
    "  \n",
    "  div = 0\n",
    "  n_items = len(item_ids)\n",
    "  \n",
    "  df_embed = item_embed[item_embed['item_id'].isin(item_ids)]\n",
    "  \n",
    "  np_embed = np.array(df_embed.drop(columns='item_id'))\n",
    "  \n",
    "  \n",
    "  for embed_index in range(n_items):\n",
    "    for embed_index_2 in range(n_items):\n",
    "      if embed_index != embed_index_2:\n",
    "        div += np.linalg.norm(np_embed[embed_index] - np_embed[embed_index_2])\n",
    "        \n",
    "  div /= n_items * (n_items-1)\n",
    "  \n",
    "  return div\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def calculate_novelty(item_ids):\n",
    "  n_items = len(item_ids)\n",
    "  \n",
    "  df_novelty = item_counts[item_counts['item_id'].isin(item_ids)]\n",
    "  \n",
    "  nov = np.sum(df_novelty['novelty'])\n",
    "  \n",
    "  nov /= n_items\n",
    "  \n",
    "  return nov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6275ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [1, 5, 10, 20]\n",
    "\n",
    "\n",
    "def test_all_users(test_df, scores, user_embed, item_embed):\n",
    "    \n",
    "    result_append = []\n",
    "    result = []\n",
    "    \n",
    "    recset = set()\n",
    "    reclist = []\n",
    "\n",
    "    coverage_dict_set = {i:set() for i in Ks}\n",
    "    coverage_dict_list = {i:list() for i in Ks}\n",
    "\n",
    "    testset = set()\n",
    "\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)), 'F1': np.zeros(len(Ks)), 'diversity': np.zeros(len(Ks)), 'novelty': np.zeros(len(Ks)), 'coverage': np.zeros(len(Ks))}\n",
    "\n",
    "    test_users = test_df['user_id'].unique().tolist()\n",
    "\n",
    "    test_items = item_embed['item_id'].tolist()\n",
    "\n",
    "    testset.update(test_items)\n",
    "\n",
    "    for u in test_users:\n",
    "        precision, recall, ndcg, hit_ratio, F1, diversity, novelty = [], [], [], [], [], [], []\n",
    "        r = []\n",
    "        user_pos_test = test_df[test_df['user_id']==u]['item_id'].tolist()\n",
    "        K_max_item_score = scores[scores['user_id']==u]['item_id'].tolist()[:20]\n",
    "        \n",
    "        for i in K_max_item_score:\n",
    "            reclist.append(i)\n",
    "            if i in user_pos_test:\n",
    "                r.append(1)\n",
    "            else:\n",
    "                r.append(0)\n",
    "                \n",
    "                \n",
    "        for K in Ks:\n",
    "        \n",
    "            metric_K = Metrics(r, K, len(user_pos_test))\n",
    "\n",
    "            precision.append(metric_K.precision_at_k())\n",
    "            recall.append(metric_K.recall_at_k())\n",
    "            ndcg.append(metric_K.ndcg_at_k())\n",
    "            hit_ratio.append(metric_K.hit_at_k())\n",
    "            F1.append(metric_K.F1(metric_K.precision_at_k(), metric_K.recall_at_k()))\n",
    "            diversity.append(calculate_diversity(K_max_item_score[:K], item_embed))\n",
    "            novelty.append(calculate_novelty(K_max_item_score[:K]))\n",
    "            coverage_dict_list[K].extend(K_max_item_score[:K])\n",
    "        \n",
    "        result_append.append({'recall': np.array(recall), 'precision': np.array(precision),\n",
    "                'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'F1': np.array(F1), \n",
    "                       'diversity': np.array(diversity), 'novelty': np.array(novelty)})\n",
    "        \n",
    "    for re in result_append:\n",
    "      result['precision'] += re['precision']/len(test_users)\n",
    "      result['recall'] += re['recall']/len(test_users)\n",
    "      result['ndcg'] += re['ndcg']/len(test_users)\n",
    "      result['hit_ratio'] += re['hit_ratio']/len(test_users)\n",
    "      result['F1'] += re['F1']/len(test_users)\n",
    "      result['diversity'] += re['diversity']/len(test_users)\n",
    "      result['novelty'] += re['novelty']/len(test_users)\n",
    "    \n",
    "    recset.update(reclist)\n",
    "  \n",
    "    for i,K in enumerate(Ks):\n",
    "        coverage_dict_set[K].update(coverage_dict_list[K])\n",
    "        result['coverage'][i] = len(coverage_dict_set[K]) / len(testset)\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df['K'] = Ks\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into high and low activity groups\n",
    "user_counts_data = train.groupby('user_id',as_index=False).agg({'item_id':'count'}).rename(columns={'item_id':'count'})\n",
    "item_counts_data = train.groupby('item_id',as_index=False).agg({'user_id':'count'}).rename(columns={'user_id':'count'})\n",
    "\n",
    "user_cutoff = user_counts_data['count'].quantile(0.8)\n",
    "print(user_cutoff)\n",
    "\n",
    "item_cutoff = item_counts_data['count'].quantile(0.8)\n",
    "print(item_cutoff)\n",
    "\n",
    "user_low_activity = user_counts_data[user_counts_data['count'] <= user_cutoff]['user_id'].tolist()\n",
    "\n",
    "item_low_activity = item_counts_data[item_counts_data['count'] <= item_cutoff]['item_id'].tolist()\n",
    "\n",
    "user_low_activity_test = test[test['user_id'].isin(user_low_activity)]\n",
    "user_high_activity_test = test[~test['user_id'].isin(user_low_activity)]\n",
    "\n",
    "item_low_activity_test = test[test['user_id'].isin(item_low_activity)]\n",
    "item_high_activity_test = test[~test['user_id'].isin(item_low_activity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e04d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "results_all = test_all_users(test, scores_df, user_embed, item_embed)\n",
    "print(time.time() - t1)\n",
    "results_all.to_csv(os.path.join(results_path,'results_all.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "results_low_user = test_all_users(user_low_activity_test, scores_df, user_embed, item_embed)\n",
    "print(time.time() - t1)\n",
    "results_low_user.to_csv(os.path.join(results_path,'results_low_user.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "results_high_user = test_all_users(user_high_activity_test, scores_df, user_embed, item_embed)\n",
    "print(time.time() - t1)\n",
    "results_high_user.to_csv(os.path.join(results_path,'results_high_user.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "results_low_item = test_all_users(item_low_activity_test, scores_df, user_embed, item_embed)\n",
    "print(time.time() - t1)\n",
    "results_low_item.to_csv(os.path.join(results_path,'results_low_item.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8862be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "results_high_item = test_all_users(item_high_activity_test, scores_df, user_embed, item_embed)\n",
    "print(time.time() - t1)\n",
    "results_high_item.to_csv(os.path.join(results_path,'results_high_item.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282494b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
